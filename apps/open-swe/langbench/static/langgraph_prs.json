[
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/5243",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/5243",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/5243.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/5243.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 5243,
    "merge_commit_sha": "08372635424fd9e2957d763cb0f2f466da552141",
    "title": "feat(langgraph): new context api (replacing `config['configurable']` and `config_schema`)",
    "body": "## Overview\r\n\r\nThis PR introduces a new API that provides a cleaner, more type-safe way to pass runtime context to LangGraph nodes/tasks. It replaces the current pattern of using `config['configurable']` and `config_schema` with a dedicated `context` parameter and wrapper `Runtime` object.\r\n\r\n## What's Changed\r\n\r\n### Before/After: Basic Context Usage\r\n\r\n### Before (Old Pattern)\r\n```python\r\nfrom langchain_core.runnables import RunnableConfig\r\n\r\ndef node(state: State, config: RunnableConfig):\r\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\r\n    return {\"result\": f\"Hello {user_id}\"}\r\n\r\ngraph.invoke(input_data, config={\"configurable\": {\"user_id\": \"123\"}})\r\n```\r\n\r\n### After (New Pattern)\r\n```python\r\nfrom dataclasses import dataclass\r\nfrom langgraph.runtime import Runtime\r\n\r\n@dataclass\r\nclass ContextSchema:\r\n    user_id: str\r\n\r\ndef node(state: State, runtime: Runtime[ContextSchema]):\r\n    user_id = runtime.context.user_id\r\n    return {\"result\": f\"Hello {user_id}\"}\r\n\r\ngraph.invoke(input_data, context={\"user_id\": \"123\"})\r\n```\r\n\r\nOR, you can use the `get_runtime` method:\r\n```python\r\nfrom langgraph.runtime import get_runtime\r\n\r\ndef node(state: State):\r\n    user_id = get_runtime(ContextSchema).context.user_id\r\n    return {\"result\": f\"Hello {user_id}\"}\r\n```\r\n\r\n<details>\r\n<summary>Before/After: Store and Stream Writer Access</summary>\r\n\r\n### Before (Old pattern)\r\n```py\r\nfrom langgraph.store.base import BaseStore\r\nfrom langchain_core.runnables import RunnableConfig\r\n\r\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\r\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\r\n    namespace = (user_id, \"memories\")\r\n    memory_id = str(uuid.uuid4())\r\n    store.put(namespace, memory_id, {\"memory\": memory})\r\n```\r\n\r\n### After (new pattern)\r\n```py\r\nfrom langgraph.runtime import Runtime\r\n\r\ndef update_memory(state: MessagesState, runtime: Runtime[ContextSchema]):\r\n    user_id = runtime.context.user_id\r\n    namespace = (user_id, \"memories\")\r\n    memory_id = str(uuid.uuid4())\r\n    runtime.store.put(namespace, memory_id, {\"memory\": memory})\r\n```\r\n</details>\r\n\r\n## Key Benefits\r\n- **Type Safety**: Type checked `context` input to `invoke` / `stream`, plus typed access to `Runtime` attributes\r\n- **Cleaner API**: Direct `context` parameter instead of nested `config['configurable']`\r\n- **Better DX**: IDE autocomplete for context fields and `Runtime` attributes\r\n- **Unified Runtime**: Single `Runtime` object provides access to context, store, and stream writer, with room for expanding to streamlined config/checkpoint information in the near future.\r\n\r\n## Breaking Changes & Migration\r\n\r\n### Deprecated APIs\r\n- `StateGraph(..., config_schema=X)` -> `StateGraph(..., context_schema=X)`\r\n- `Pregel.config_schema` → `Pregel.get_context_jsonschema()` - this is largely meant to be external and we don't anticipate this affecting many users\r\n\r\n### Migration Details\r\n- Maintains backward compatibility with existing `config['configurable']` usage\r\n- Deprecation warnings guide users to the new API\r\n\r\n## Future Work\r\n\r\n- [ ] Deprecate injection pattern for `store`, `stream_writer`, maybe `previous`, update docs to recommend popping from runtime.\r\n- [ ] Support `Runtime` injection for tools, right now only `get_runtime` is supported\r\n- [ ] LangGraph style guide with recommended best practices\r\n\r\nEventually, I think we should move away from storing and popping things from `config[\"configurable\"]`, which can be a fully internal refactor. Though I would love to do this pre v1, it should largely be internal, so can be done afterwards. Config changes should be in a different PR than this one to keep things reasonably scoped. This PR is already pretty big. Lots of plumbing.\r\n\r\n## Related Issues\r\nCloses #5023\r\n",
    "created_at": "2025-06-28T01:04:08Z",
    "merged_at": "2025-07-15T13:20:20Z",
    "pre_merge_commit_sha": "e0bf4a7bc35d9bb9b9c52a0c652446ff9c9734ba",
    "tests": {
      "libs/langgraph/tests/test_deprecation.py": ["test_config_schema_deprecation"],
      "libs/langgraph/tests/test_pregel.py": ["test_context_json_schema"],
      "libs/langgraph/tests/test_runnable.py": ["test_runnable_callable_func_accepts"],
      "libs/langgraph/tests/test_runtime.py": ["test_injected_runtime", "test_context_runtime"]
    },
    "test_files": [
      "libs/langgraph/tests/test_deprecation.py",
      "libs/langgraph/tests/test_pregel.py",
      "libs/langgraph/tests/test_runnable.py",
      "libs/langgraph/tests/test_runtime.py"
    ]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/4374",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/4374",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/4374.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/4374.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 4374,
    "merge_commit_sha": "4eb124e83de865d9bbff83212020e14ddea54465",
    "title": "[breaking]: Improve interrupt behavior when `stream_mode='values'`",
    "body": "This PR does a few things:\r\n1. Surfaces interrupts when `stream_mode='values'` (particularly relevant for `invoke`, where this is the default behavior) \r\n2. Adds an `interrupt_id` property to the `Interrupt` dataclass so that interrupts can effectively be mapped to resumes\r\n3. Minor docs updates to reflect the new pattern (no need for a special section on interrupts with `invoke` and `ainvoke`)\r\n\r\n* In a different PR (the one with the multiple resume values), as it's more relevant there: add an `interrupts` property to `StateSnapshot` so that `interrupts` can easily be iterated over if users are attempting to map interrupts to resumes.\r\n\r\nI **don't** recommend we release this until we have multi-resumes working.\r\n\r\n## Example\r\n\r\nWe have the following setup where we're sending multiple prompts to the child graph, which uses `interrupt`:\r\n\r\n```py\r\ndef child_graph(state):\r\n    human_input = interrupt(state[\"prompt\"])\r\n\r\n    return {\r\n        \"human_inputs\": [human_input],\r\n    }\r\n```\r\n\r\n<img width=\"142\" alt=\"Screenshot 2025-04-23 at 10 01 12 AM\" src=\"https://github.com/user-attachments/assets/c6238bf1-54ad-4e48-ab0b-60a0bfc18485\" />\r\n\r\nOld behavior:\r\n\r\n```py\r\ninitial_input = {\"prompts\": [\"a\", \"b\"]}\r\n\r\nprint(parent_graph.invoke(input=initial_input,config=thread_config,stream_mode=\"values\"))\r\n#> {'prompts': ['a', 'b'], 'human_inputs': []}\r\n\r\nprint(parent_graph.invoke(Command(resume=\"hello 1\"),config=thread_config,stream_mode=\"values\"))\r\n#> {'prompts': ['a', 'b'], 'human_inputs': ['hello 1']}\r\n\r\nprint(parent_graph.invoke(Command(resume=\"hello 2\"),config=thread_config,stream_mode=\"values\"))\r\n#> {'prompts': ['a', 'b'], 'human_inputs': ['hello 1', 'hello 2']}\r\n```\r\n\r\nNew behavior:\r\n\r\n```py\r\ninitial_input = {\"prompts\": [\"a\", \"b\"]}\r\n\r\nprint(parent_graph.invoke(input=initial_input,config=thread_config,stream_mode=\"values\"))\r\n\"\"\"\r\n{\r\n  \"prompts\": [\"a\", \"b\"],\r\n  \"human_inputs\": [],\r\n  \"__interrupt__\": [\r\n    Interrupt(\r\n      value=\"a\",\r\n      resumable=True,\r\n      ns=[\"child_graph:38d43a18-a5e7-8ab2-ca83-9d80f6e9ca83\"]\r\n    ),\r\n    Interrupt(\r\n      value=\"b\",\r\n      resumable=True,\r\n      ns=[\"child_graph:dad810e8-738e-9f90-41cd-30c0091eb79b\"]\r\n    )\r\n  ]\r\n}\r\n\"\"\"\r\n\r\nprint(parent_graph.invoke(Command(resume=\"hello 1\"),config=thread_config,stream_mode=\"values\"))\r\n\"\"\"\r\n{\r\n  \"prompts\": [\"a\", \"b\"],\r\n  \"human_inputs\": [\"hello 1\"],\r\n  \"__interrupt__\": [\r\n    Interrupt(\r\n      value=\"b\",\r\n      resumable=True,\r\n      ns=[\"child_graph:dad810e8-738e-9f90-41cd-30c0091eb79b\"]\r\n    )\r\n  ]\r\n}\r\n\"\"\"\r\n\r\nprint(parent_graph.invoke(Command(resume=\"hello 2\"),config=thread_config,stream_mode=\"values\"))\r\n#> {'prompts': ['a', 'b'], 'human_inputs': ['hello 1', 'hello 2']}\r\n```",
    "created_at": "2025-04-22T17:17:21Z",
    "merged_at": "2025-04-24T15:21:28Z",
    "pre_merge_commit_sha": "b81c21f311fd131d5969c33d238be2eeed5cc522",
    "tests": {
      "libs/langgraph/tests/test_large_cases.py": ["test_dynamic_interrupt"],
      "libs/langgraph/tests/test_pregel.py": ["test_subgraph_checkpoint_true_interrupt"],
      "libs/langgraph/tests/test_pregel_async.py": ["test_dynamic_interrupt_subgraph"]
    },
    "test_files": [
      "libs/langgraph/tests/test_large_cases.py",
      "libs/langgraph/tests/test_pregel.py",
      "libs/langgraph/tests/test_pregel_async.py"
    ]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/3126",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/3126",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/3126.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/3126.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 3126,
    "merge_commit_sha": "a37c4d6f4928a3e1d91f2061fc6af142b17e0408",
    "title": "langgraph[patch]: allow ToolNode to accept ToolCalls",
    "body": "Alternative to https://github.com/langchain-ai/langgraph/pull/3124\r\n\r\nCurrently if a tool interrupts, the entire tool node executes again after resuming. So tools can get executed twice if parallel tool calls are generated. Here we allow ToolNode to accept tool calls, so we can use the `Send` API to distribute the tool calls to multiple instances of the tool node.\r\n\r\n```python\r\nfrom langchain_anthropic import ChatAnthropic\r\nfrom langchain_core.tools import tool\r\nfrom langgraph.checkpoint.memory import MemorySaver\r\nfrom langgraph.prebuilt import create_react_agent\r\nfrom langgraph.types import Command, Send, interrupt\r\n\r\n\r\n@tool\r\ndef human_assistance(query: str) -> str:\r\n    \"\"\"Request assistance from a human.\"\"\"\r\n    human_response = interrupt({\"query\": query})\r\n    return human_response[\"data\"]\r\n\r\n\r\n@tool\r\ndef get_weather(location: str) -> str:\r\n    \"\"\"Use this tool to get the weather.\"\"\"\r\n    return \"It's sunny!\"\r\n\r\n\r\ntools = [get_weather, human_assistance]\r\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\r\n\r\nagent = create_react_agent(\r\n    llm,\r\n    tools,\r\n    checkpointer=MemorySaver(),\r\n    tool_call_parallelism=\"parallel_tool_nodes\",\r\n)\r\n\r\n\r\nuser_input = (\r\n    \"Could you please (1) request assistance for building an AI agent \"\r\n    \"from a human, and (2) search for the weather in Boston, MA? \"\r\n    \"Generate two tool calls at once.\"\r\n)\r\n\r\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\r\n\r\nfor event in agent.stream(\r\n    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\r\n    config,\r\n    stream_mode=\"values\",\r\n):\r\n    event[\"messages\"][-1].pretty_print()\r\n```\r\n```\r\n...\r\n```\r\n```python\r\nhuman_response = \"You should check out LangGraph to build your agent.\"\r\nhuman_command = Command(resume={\"data\": human_response})\r\n\r\nfor event in agent.stream(human_command, config, stream_mode=\"values\"):\r\n    event[\"messages\"][-1].pretty_print()\r\n```",
    "created_at": "2025-01-21T17:58:50Z",
    "merged_at": "2025-01-31T17:20:59Z",
    "pre_merge_commit_sha": "4b3e07b67aa5a992531cab169286c3cda0c38a0a",
    "tests": {
      "libs/langgraph/tests/test_prebuilt.py": ["test_no_prompt", "test_model_with_tools", "test_react_agent_with_structured_response", "test_tool_node_tool_call_input", "test_tool_node_node_interrupt"]
    },
    "test_files": ["libs/langgraph/tests/test_prebuilt.py"]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/3095",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/3095",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/3095.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/3095.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 3095,
    "merge_commit_sha": "444faec6e6c635b6ee343da806b2bbd4a7bbce6b",
    "title": "Fix two issues with task/stream timing",
    "body": "- both issues are related to the fact that waiters for futures are notified of completion before \"done\" callbacks are called\r\n- 1st issue manifested as interrupt stream event being emitted before the result of a task that logically finished first (it's in the line above in body of the entrypoint function) -> this is solved by always returning to use code a fresh future chained on the original future, because chaining is done via done callbacks (therefore the chained future will only resolve after done callbacks of the original feature are called)\r\n- 2nd issue mainfested as sometimes (very rarely) the last stream event not being printed before stream() finishes. this is solved by ensuring we only return out of PregelRunner.tick() once all \"done\" callbacks are called, previously we were approximating this through use of asyncio.sleep(0) / time.sleep(0). The new solution instead waits on a threading/asyncio.Event which will only be set by the last \"done\" callback to fire\r\n- this PR also disables incomplete support for calling sync tasks from async entrypoints",
    "created_at": "2025-01-17T23:35:26Z",
    "merged_at": "2025-01-17T23:44:52Z",
    "pre_merge_commit_sha": "e4a5c8fd28ceca30171073aebd64b802699c54ba",
    "tests": {
      "libs/langgraph/tests/test_pregel_async.py": ["test_step_timeout_on_stream_hang"]
    },
    "test_files": ["libs/langgraph/tests/test_pregel_async.py"]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/5801",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/5801",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/5801.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/5801.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 5801,
    "merge_commit_sha": "2920a9dd197e75554720dae3e0c6bebb638fa621",
    "title": "fix(langgraph): Tidy up `AgentState`",
    "body": "Fixes https://github.com/langchain-ai/langgraph/issues/5784\r\n\r\n* Removes usage of `is_last_step`, no longer needed with `remaining_steps`\r\n* Make `remaining_steps` `NotRequired` so that json schema doesn't suggest need for user input\r\n* Move `PregelScratchpad` to shared utils file to prevent circular import issue (it's used from `channels/managed` and other pregel files).\r\n* Ensures that managed values wrapped in `NotRequired` or `Required` are still recognized!",
    "created_at": "2025-08-01T18:31:32Z",
    "merged_at": "2025-08-03T11:12:54Z",
    "pre_merge_commit_sha": "db8ed4e9e424ed29c8165602f17ee800c671681b",
    "tests": {
      "libs/langgraph/tests/test_managed_values.py": ["test_managed_values_recognized"]
    },
    "test_files": ["libs/langgraph/tests/test_managed_values.py"]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/5796",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/5796",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/5796.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/5796.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 5796,
    "merge_commit_sha": "220314b53a960964a82657451b846f3a7cb2f348",
    "title": "fix(langgraph): fix up deprecation warnings",
    "body": "Fixes https://github.com/langchain-ai/langgraph/issues/5795\r\n\r\n* Must use `category=None` on decorator so that we get type checking support but no dupe warning\r\n* Fixed tuple on `config_type` warning causing false warning",
    "created_at": "2025-08-01T14:27:51Z",
    "merged_at": "2025-08-01T14:33:46Z",
    "pre_merge_commit_sha": "38bbd92e01d8437b70c45355b6005fa40c204844",
    "tests": {
      "libs/langgraph/tests/test_deprecation.py": ["test_config_schema_deprecation_on_entrypoint"]
    }
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/5708",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/5708",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/5708.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/5708.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 5708,
    "merge_commit_sha": "7436777e7d5ebdea1a7787755b71c75faca9885f",
    "title": "fix(langgraph): enforce config injection even when optional",
    "body": "Fixes https://github.com/langchain-ai/langgraph/issues/5698\r\n\r\nI would like to do a more general refactor of this logic at some point as well using typing introspection utilities. Claude code first pass: https://github.com/langchain-ai/langgraph/pull/5709",
    "created_at": "2025-07-29T19:14:10Z",
    "merged_at": "2025-07-29T20:11:37Z",
    "pre_merge_commit_sha": "479373bd81f538b81362ae8bea9d1b6923e1f60e",
    "tests": {
      "libs/langgraph/tests/test_runnable.py": ["test_config_injection"]
    },
    "test_files": ["libs/langgraph/tests/test_runnable.py"]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/3037",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/3037",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/3037.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/3037.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 3037,
    "merge_commit_sha": "aab6fdf3f3c5ff6f695335cc0896b32da4b1dc6e",
    "title": "Fix Send order after interrupt/resume",
    "body": "- order was incorrectly based on task id, instead of the correct task path\r\n- this requires storing task paths on checkpointers\r\n- addition of task_path to put_writes is made backwards compatible by checking signature on call, and treating it as an optional arg",
    "created_at": "2025-01-15T02:12:35Z",
    "merged_at": "2025-01-15T19:42:08Z",
    "pre_merge_commit_sha": "0adbd89d9aaad57e8e4431f308c4372a740e4cbf",
    "tests": {
      "libs/langgraph/tests/test_algo.py": ["test_tuple_str"],
      "libs/langgraph/tests/test_large_cases.py": ["test_send_dedupe_on_resume"],
      "libs/langgraph/tests/test_pregel_async.py": ["test_send_dedupe_on_resume"]
    },
    "test_files": [
      "libs/langgraph/tests/test_algo.py",
      "libs/langgraph/tests/test_large_cases.py",
      "libs/langgraph/tests/test_pregel_async.py"
    ]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/2393",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/2393",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/2393.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/2393.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 2393,
    "merge_commit_sha": "29f833b1a77397adab5e13e953f7e9b48f4a0174",
    "title": "lib: Add interrupt() function",
    "body": "- This works similarly to the input() function from stdlib\r\n- calling it in a node interrupts execution\r\n- invoking the graph with Command(resume=...) will set ... as the return value of interrupt() so that the node can access the \"answer\" to the \"question\"\r\n- This PR also starts the work to control the graph on invoke/stream with Command() input, to be continued in a future PR\r\n\r\n```py\r\n    class State(TypedDict):\r\n        my_key: Annotated[str, operator.add]\r\n        market: str\r\n\r\n    async def tool_two_node(s: State) -> State:\r\n        if s[\"market\"] == \"DE\":\r\n            answer = interrupt(\"Just because...\")\r\n        else:\r\n            answer = \" all good\"\r\n        return {\"my_key\": answer}\r\n\r\n    tool_two_graph = StateGraph(State)\r\n    tool_two_graph.add_node(\"tool_two\", tool_two_node)\r\n    tool_two_graph.add_edge(START, \"tool_two\")\r\n    tool_two = tool_two_graph.compile()\r\n\r\n        tool_two = tool_two_graph.compile(checkpointer=checkpointer)\r\n\r\n        # flow: interrupt -> resume with answer\r\n        thread2 = {\"configurable\": {\"thread_id\": \"2\"}}\r\n        # stop when about to enter node\r\n        assert [\r\n            c\r\n            async for c in tool_two.astream(\r\n                {\"my_key\": \"value ⛰️\", \"market\": \"DE\"}, thread2\r\n            )\r\n        ] == [\r\n            {\"__interrupt__\": [Interrupt(value=\"Just because...\", when=\"during\")]},\r\n        ]\r\n        # resume with answer\r\n        assert [\r\n            c async for c in tool_two.astream(Command(resume=\" my answer\"), thread2)\r\n        ] == [\r\n            {\"tool_two\": {\"my_key\": \" my answer\"}},\r\n        ]\r\n```",
    "created_at": "2024-11-12T01:45:14Z",
    "merged_at": "2024-11-13T21:34:20Z",
    "pre_merge_commit_sha": "7a3ea427432dd5e8f4ee101a8c773f3afbc3214c",
    "tests": {
      "libs/langgraph/tests/test_pregel.py": ["test_send_react_interrupt_control"],
      "libs/langgraph/tests/test_pregel_async.py": ["test_dynamic_interrupt"]
    },
    "test_files": [
      "libs/langgraph/tests/test_pregel.py",
      "libs/langgraph/tests/test_pregel_async.py"
    ]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/1735",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/1735",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/1735.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/1735.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 1735,
    "merge_commit_sha": "c4d4d61a43877419ffb51f8077ca6217ffda2a18",
    "title": "Stream subgraph output while it executes",
    "body": "- previous behavior was to buffer all output from subgraph until it finished, now subgraph steps are emitted as soon as produced, while the subgraph is still running\r\n- this is slightly slower in benchmark scripts, but worth it as it's much \"faster\" in real-world latency",
    "created_at": "2024-09-17T01:04:29Z",
    "merged_at": "2024-09-17T17:14:26Z",
    "pre_merge_commit_sha": "f59435a892e96fb9092e0055a6df2e1dfc5111a9",
    "tests": {
      "libs/langgraph/tests/test_pregel_async.py": ["test_stream_subgraph_output"]
    },
    "test_files": ["libs/langgraph/tests/test_stream_subgraphs_during_execution.py"]
  },
  {
    "url": "https://api.github.com/repos/langchain-ai/langgraph/pulls/1630",
    "html_url": "https://github.com/langchain-ai/langgraph/pull/1630",
    "diff_url": "https://github.com/langchain-ai/langgraph/pull/1630.diff",
    "patch_url": "https://github.com/langchain-ai/langgraph/pull/1630.patch",
    "repo_owner": "langchain-ai",
    "repo_name": "langgraph",
    "pr_number": 1630,
    "merge_commit_sha": "e3ca7bb3e9d34b09633852f4d08d55f6dcd4364b",
    "title": "Implement LangGraph Scheduler for Kafka",
    "body": "- Orchestrator and Executor classes to run LangGraph in a distributed fashion using Kafka as a message bus for communication\r\n- Orchestrator and Executor run on-demand when a new message is published to the topic they listen to\r\n- Orchestrator is responsible for running the Pregel algorithm (deciding next tasks to run) and sending messages to the executor topic\r\n- Executor is responsible for executing each task (node), and sending messages to the orchestrator topic when done",
    "created_at": "2024-09-06T01:01:06Z",
    "merged_at": "2024-09-11T00:31:59Z",
    "pre_merge_commit_sha": "34d530d5d83837fa080c1db2d055fe952cfc8488",
    "tests": {
      "libs/langgraph/tests/test_pregel.py": ["test_invoke_two_processes_in_out_interrupt"],
      "libs/langgraph/tests/test_pregel_async.py": ["test_pending_writes_resume"]
    },
    "test_files": [
      "libs/langgraph/tests/test_pregel.py",
      "libs/langgraph/tests/test_pregel_async.py"
    ]
  }
]